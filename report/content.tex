\section{Introduction}
In this work we attempt the reproduction of the results presented in the paper "Dynamic Routing Transformer Network for Multimodal Sarcasm Detection" \cite{tian-etal-2023-dynamic}, which proposes DynRT, a new architecture to tackle the sarcasm detection task. The authors of the paper leverage multimodal NLP to join information obtained from pictures and their corresponding captions. 

\section{Scope of reproducibility}
\label{sec:claims}

The authors of the paper claim that their model outperforms the state-of-the-art models in the sarcasm detection task. They also claim that their model is able to leverage multimodal information to improve the overall performance in terms of both accuracy ad F1 score.
The novel DynRT architecture is used to capture incongruity between the picture and the caption, which is a key aspect of sarcasm, and is tested on a multimodal sarcasm detection dataset to compare the results with other state-of-the-art models.
 


%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

\section{Methodology}
To reproduce the results stated in the original paper we mainly used the code made available by the authors in their public repository (TODO: link). The instructions provided were partially incomplete, and the code needed some small adjustments. We used a Colab notebook to run the experiments, which allowed us to leverage the NVIDIA V100 GPU.

\subsection{Model description}
The DynRT model is composed of three main parts: two encoders and a dynamic routing transformer. The two encoders are used to encode the input text and the image features, and are respectively RoBERTa (TODO: cite) for the text and ViT (TODO: cite) for the images. The dynamic routing transformer is used to capture the incongruity between the two modalities. It is composed of a stack of $N$ layers, each of which is composed of a multi-head co-attention routing module, a multi-head self-attention module and a feed-forward network; each module is followed by a residual connection and a normalization layer. The transformer leverages hierarchical co-attention to module the incongruity between texts and images. The output of the encoders is fed to the dynamic routing transformer, which outputs a vector of \textit{routed features} that are used for the classification together with the \textit{image features}. The output of the classification is a vector containing the probabilities of the input being sarcastic or not. 

\subsection{Datasets}
The dataset used to evaluate the architecture is the Multimodal Sarcasm Detection (MSD) dataset (TODO: cite), whose composition and split information are shown in table \ref{tab:MSD}.
\begin{table}
    \caption{MSD dataset composition and split information}
    \label{tab:MSD}
    \centering
    \begin{tabular}{|c c c c|}
        \hline
         & \textbf{Train} & \textbf{Development} & \textbf{Test} \\
        \hline
        Sarcastic & 8,642 & 959 & 959 \\
        Non-sarcastic & 11,174 & 1,451 & 1,450 \\
        Total & 19,816 & 2,410 & 2,409 \\
        \hline
    \end{tabular}
\end{table}
The dataset is not perfectly balanced, but it reflects the real-world distribution of sarcastic and non-sarcastic sentences. The dataset is composed of sentences and their corresponding images. The preprocessing operations performed on the dataset before feeding it to the model are the following:
\begin{itemize}
    \item Discard tweets with regular worlds, such as \textit{sarcasm, sarcastic, irony, jokes, humor, reposting, ironic, exgag}, and URLs
    \item replace mentions with the token \textit{<USER>}
\end{itemize}
The dataset can be downloaded from https://github.com/headacheboy/data-of-multimodal-sarcasm-detection.


\subsection{Hyperparameters}
The configuration used to run the experiments is pretty similar to the one used by the authors (i.e. the hyperparameters that could have influenced the performance of the model, such as the learning rate, the batch size, etc., were left unchanged) in order to perform a fair comparison of the results. The only difference in our runs was in the parallelization of the training process, which was not configured by default. Initially we tried without it, but the training process was excessively long. Therefore, we increased the number of workers to 10. In this way we could run the whole training process (15 epochs) in less than 4 hours. 

\subsection{Experimental setup and code}
Since we used a Colab notebook to run the experiments, we created a copy of our repository in a Google Drive folder, and we mounted the drive in the notebook. Then we tried installing the requirements in the way described in the readme.md of the original repository (TODO: link) but we encountered some problems due to incompatible versions of some libraries. Therefore, we manually changed the installation process by updating the problematic libraries to their latest version.
We then proceeded to the preprocessing phase as shown by the authors, which consisted in performing the cleaning operations on the dataset and in converting the images to tensor. This last operation was performed locally, since we encountered some issues when trying to run it on the Colab notebook. We then uploaded the preprocessed dataset to the notebook and we started the training process. Here, we encountered other issues. Firstly, we had no feedback about how the training process was proceeding, therefore we decided to increase the verbosity by adding a code line that printed the advancement of the number of batches processed (with the default batch size of 32 we had to elaborate 611 batches for each epoch). Secondly, we got an exception due to the loading of some .npy files containing the tensors (previosly created starting from the images in the preprocessing phase). Specificaly, some files couldn't be serialized in the way the algorithm was originally written. To solve this, we added an exception handling code block to manage these files. Finally, since Colab's stability is not complety reliable, we added a simple script in our notebook that leverages the checkpoint system implemented by the original authors: it checks whether a checkpoint file already exists or not, and if it does it loads the model from it (in case of multiple checkpoints models, it load the last one).
After these changes, we were able to run the training process without any other issues.


\subsection{Computational requirements}
As already mentioned, we used the NVIDIA V100 GPU provided by Colab Pro to run the experiments. The whole training and evaluation process took about 4 hours to complete, and we experienced a GPU memory consumption of about 4 GB. It is interesting to note how with no parallelization a single training epoch took about 3 hours to complete, which would have led to an excessively long training process of 45 hours for 15 epochs. This would have been an issue since Colab only allows for 12 hours of continuous execution.

\section{Results}
\label{sec:results}

\subsection{Results reproducing original paper}
The results obtained are in line with the ones provided in the paper. The authors report accuracy and F1 metrics obtained by averaging the performance of five different runs, showing how the newly introducted architecture outperforms other state-of-the-art models. The results that we obtained, shown in table (TODO: table) confirm those claims for accuracy results, but the F1 score obtained by us is slightly lower. 

\begin{table}
    \caption{Results obtained by the original authors and by us}
    \label{tab:results}
    \centering
    \begin{tabular}{| c|c c|}
        \hline
         & \textbf{Original} & \textbf{Reproduced} \\
        \hline
         \textbf{Accuracy} & 93.49 & 93.59 \\
        \hline
        \textbf{F1} & 93.21 & 91.93 \\
        \hline
    \end{tabular}
\end{table}

\section{Discussion}

TODO: talk about how the learning process was increasingly getting better (it had a spike)
Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was easy}
Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 

Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers). 

\subsection{What was difficult}
List part of the reproduction study that took more time than you anticipated or you felt were difficult. 

Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow". 

\subsection{Communication with original authors}
Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback before it gets published. 
